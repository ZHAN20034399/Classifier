{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import shap\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from typing import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib\n",
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "file_path = \"D:/cddvd/sedex_vms_zhong.xlsx\"  # Please enter the path to the Supplementary data 3\n",
    "data = pd.read_excel(file_path)\n",
    "df = data[['Deposit type', 'Co', 'Ni', 'Zn', 'Cd', 'Sb', 'Pb', 'Ag', 'Se']]\n",
    "X = df.copy(deep=True)\n",
    "y = X.pop('Deposit type')\n",
    "\n",
    "# Print class distribution\n",
    "print(y.value_counts())\n",
    "y_int, index = pd.factorize(y, sort=True)\n",
    "y = y_int\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Models to be used for training\n",
    "models = (\n",
    "    xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    ")\n",
    "\n",
    "# Cross-validation scores for each model\n",
    "for clf in models:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "    print(f'{scores.mean():2.2f} ± {scores.std():2.2f}')\n",
    "\n",
    "# Create a pipeline with XGBoost\n",
    "pipe_clf = make_pipeline(xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))\n",
    "\n",
    "# Define a custom normalizer class\n",
    "class MidpointNormalize(Normalizer):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        super().__init__(vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "# Set ranges for max_depth and learning_rate parameters for XGBoost\n",
    "param_grid = {\n",
    "    \"xgbclassifier__max_depth\": [3, 6, 9],\n",
    "    \"xgbclassifier__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"xgbclassifier__n_estimators\": [50, 100, 200],\n",
    "    \"xgbclassifier__subsample\": [0.8, 1.0],\n",
    "    \"xgbclassifier__colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid = GridSearchCV(pipe_clf, param_grid=param_grid, cv=10, scoring=\"f1_macro\", n_jobs=-1, refit=True)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"The best parameters are {grid.best_params_} with a score of {grid.best_score_:0.2f}\")\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"\\nCross-validation scores:\")\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid.cv_results_['params']):\n",
    "    print(f\"{mean:0.3f} (+/-{std*2:0.03f}) for {params}\")\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred = grid.predict(X_test)\n",
    "t_train_pred = grid.predict(X_train)\n",
    "y_test_proba = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Print classification report for the training set\n",
    "print(\"Training set report:\")\n",
    "print(classification_report(y_train, t_train_pred))\n",
    "print(\"Test set report:\")\n",
    "print(classification_report(y_test, y_test_pred, output_dict=False))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "print(classification_report(y_test, y_test_pred, output_dict=False))\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained XGBoost model from the grid search\n",
    "xgb_model = grid.best_estimator_.named_steps[\"xgbclassifier\"]\n",
    "\n",
    "# Define a prediction function using the trained XGBoost model\n",
    "def predict_function(X):\n",
    "    return xgb_model.predict_proba(X)\n",
    "\n",
    "# Create a SHAP explainer using KernelExplainer\n",
    "explainer = shap.KernelExplainer(predict_function, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Plot SHAP Bee Swarm plots for each class\n",
    "for i, class_name in enumerate(['No Deposit', 'Deposit']):\n",
    "    shap_exp = shap.Explanation(\n",
    "        base_values=explainer.expected_value,\n",
    "        values=shap_values[i],\n",
    "        data=X_train,\n",
    "        feature_names=X_train.columns\n",
    "    )\n",
    "    shap.plots.beeswarm(shap_exp, max_display=10)  # Show top 10 features\n",
    "    plt.title(f'SHAP Bee Swarm Plot for {class_name}')\n",
    "    plt.savefig(f'D:/cddvd/SHAP_beeswarm_{class_name}.jpg', dpi=600, format='jpg')\n",
    "    plt.close()  # Close the current plot\n",
    "\n",
    "# Plot SHAP summary plot (showing feature impacts on predictions)\n",
    "shap.summary_plot(shap_values[1], X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label order\n",
    "label_order = [\"SEDEX\", \"VMS\"]\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "cm_df = pd.DataFrame(cm, columns=label_order, index=label_order)\n",
    "cm_df_percentage = cm_df.div(cm_df.sum(axis=0), axis=1) * 100\n",
    "plt.figure(figsize=(2.5, 2.5))\n",
    "plt.rc('font', family='Arial', size=8)\n",
    "ax = sns.heatmap(cm_df, linewidths=.5, ax=plt.gca(), cmap=\"Blues\")\n",
    "norm = plt.Normalize(vmin=cm_df.values.min(), vmax=cm_df.values.max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"Blues\", norm=norm)\n",
    "\n",
    "for i in range(len(cm_df)):\n",
    "    for j in range(len(cm_df)):\n",
    "        value = cm_df.iloc[i, j]\n",
    "        percentage = cm_df_percentage.iloc[i, j]\n",
    "        color = 'white' if sm.to_rgba(value)[0:3] < (0.5, 0.5, 0.5) else 'black'  # Choose text color based on cell color\n",
    "        plt.text(j + 0.5, i + 0.5, f\"{value}\\n{percentage:.1f}%\",\n",
    "                 ha='center', va='center', color=color, family='Arial', size=8)\n",
    "plt.title(\"Test set confusion matrix (XGBoost)\", fontsize=8)\n",
    "plt.xlabel(\"Predictions\", fontsize=8)\n",
    "plt.ylabel(\"True labels\", fontsize=8)\n",
    "\n",
    "ax.set_xticklabels(label_order, rotation=45, fontsize=8)\n",
    "ax.set_yticklabels(label_order, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'D:\\cddvd\\confusion_matrix_XGBoost_ccp.svg', dpi=600, format='svg')\n",
    "plt.savefig(r'D:\\cddvd\\confusion_matrix_XGBoost_ccp.pdf', dpi=600, format='pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"\n",
    "RF classifier to predict the genetic classes of the chalcopyrite source with \"Co\", \"Ni\", \"Zn\", \"Sb\", \"Pb\", \"Ag\", \"Se\", \"Cd\"values,\n",
    "Please enter the path of the .xlsx data file.(for example: /path/to/file/example_data.xlsx )\n",
    "The data are supposed to contain all the 8 features above for prediction.\n",
    "If any one of the features is missing in a sample, that sample will be discarded.\n",
    "The columns' names of Co, Ni, Zn, Cd, Sb, Pb, Ag, Se should be exactly as listed above without any prefix and suffix\n",
    "and MAKE SURE this column name row is the FIRST row.\n",
    "\"\"\"\n",
    ")\n",
    "data_file_path = r\"D:\\我的论文\\dongshengmiao_knn_ccp.xlsx\"#Please enter the path to the data file\n",
    "df = pd.read_excel(data_file_path)\n",
    "index = ['SEDEX', 'VMS']\n",
    "elements = [ \"Co\",\"Ni\",\"Zn\", \"Cd\", \"Sb\", \"Pb\",\"Ag\",\"Se\"]\n",
    "for element in elements:\n",
    "    df[element] = pd.to_numeric(df[element], errors=\"coerce\")\n",
    "\n",
    "to_predict = df.loc[:, elements].dropna()\n",
    "to_predict.reset_index(drop=True, inplace=True)\n",
    "print(f\"{to_predict.shape[0]} samples available\")\n",
    "print(to_predict.describe())\n",
    "predict_res = grid.predict(to_predict)\n",
    "predict_res = list(predict_res)\n",
    "for i, ind in enumerate(predict_res):\n",
    "    predict_res[i] = index[ind]\n",
    "\n",
    "c: Counter[str] = Counter(predict_res)\n",
    "if not c:\n",
    "    input(\"no sample with the 8 features detected!\")\n",
    "    raise SystemExit()\n",
    "    \n",
    "proba = grid.predict_proba(to_predict)\n",
    "predict_res = np.array(predict_res)\n",
    "predict_res = predict_res.reshape((predict_res.shape[0], 1))\n",
    "res = np.concatenate([predict_res, proba], axis=1)\n",
    "res = pd.DataFrame(res, columns=['pred_chalcopyrite_type', 'SEDEX_proba', 'VMS_proba'])\n",
    "pd.set_option('display.max_columns', 10)\n",
    "print('Detailed report preview:\\n', res)\n",
    "print(\"The samples are predicted respectively to be: \")\n",
    "print(c.most_common(), \"\\n\")\n",
    "print(\n",
    "    f\"The most possible type of the group of samples is: {c.most_common(1)[0][0]}.\\n\"\n",
    ")\n",
    "if input('Save report? (y/n): ').lower() == 'y':\n",
    "    base_filename = os.path.basename(data_file_path)\n",
    "    prefix, _ = os.path.splitext(base_filename)\n",
    "    save_name = prefix + '_resultSVMccp.xlsx'\n",
    "    res2 = pd.concat([to_predict['Pb'], res], axis=1, )\n",
    "    output = df.join(res2.set_index('Pb'), on='Pb')\n",
    "    output.to_excel(save_name)\n",
    "    print(f'{save_name} saved.')\n",
    "input(\"Press any key to exit.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
